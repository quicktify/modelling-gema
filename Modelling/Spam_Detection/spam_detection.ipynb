{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# IMPOR LIBRARY & KONFIGURASI\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library dasar dan lanjutan berhasil diimpor.\n",
      "Konfigurasi: DATASET_PATH='../../dataset/dataset_spam_nlp.csv', STEMMING=False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory # pip install Sastrawi\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory # Untuk stopwords default Sastrawi jika file tidak ada\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler # Untuk custom features numerik jika digabung\n",
    "from scipy.sparse import hstack # Untuk menggabungkan matriks sparse\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle # Untuk menyimpan model\n",
    "\n",
    "print(\"Library dasar dan lanjutan berhasil diimpor.\")\n",
    "\n",
    "# --- Konfigurasi Global ---\n",
    "DATASET_PATH = '../../dataset/dataset_spam_nlp.csv'\n",
    "SLANG_FILE_PATH = '../../dataset/slang_indo.csv' \n",
    "STOPWORDS_FILE_PATH = '../../dataset/stopwords-id.txt'\n",
    "RANDOM_STATE_SEED = 42\n",
    "TEST_SET_SIZE = 0.2\n",
    "# Opsi untuk stemming, bisa diubah menjadi True untuk mengaktifkan\n",
    "PERFORM_STEMMING_DEFAULT = False \n",
    "# Opsi untuk SMOTE (akan diimplementasikan jika diperlukan dan diputuskan)\n",
    "# USE_SMOTE = False \n",
    "\n",
    "print(f\"Konfigurasi: DATASET_PATH='{DATASET_PATH}', STEMMING={PERFORM_STEMMING_DEFAULT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# BAGIAN 2: FUNGSI-FUNGSI PEMBANTU (HELPER FUNCTIONS)\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.1. Persiapan Alat Preprocessing ---\n",
    "def setup_preprocessing_tools(slang_file_path, stopwords_file_path):\n",
    "    slang_dict = {}\n",
    "    try:\n",
    "        slang_df = pd.read_csv(slang_file_path, header=None, names=['slang', 'formal'], sep=',')\n",
    "        slang_dict = dict(zip(slang_df['slang'], slang_df['formal']))\n",
    "        print(f\"Kamus slang berhasil dimuat dari '{slang_file_path}'. Jumlah entri: {len(slang_dict)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Peringatan: File kamus slang '{slang_file_path}' tidak ditemukan.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Peringatan: Gagal memuat kamus slang. Error: {e}\")\n",
    "    \n",
    "    additional_slang = {\n",
    "        'yg': 'yang', 'dg': 'dengan', 'rt': 'rumah tangga', 'utk': 'untuk', 'cuman': 'cuma', \n",
    "        'gini': 'begini', 'gitu': 'begitu', 'tdk': 'tidak', 'ga': 'tidak', 'gak': 'tidak', \n",
    "        'nggak': 'tidak', 'gk': 'tidak', 'dr': 'dari', 'dlm': 'dalam', 'jd': 'jadi', \n",
    "        'jgn': 'jangan', 'sdh': 'sudah', 'blm': 'belum', 'bgt': 'banget', 'jg': 'juga', \n",
    "        'lg': 'lagi', 'sm': 'sama', 'tp': 'tapi', 'udah': 'sudah', 'sy': 'saya', \n",
    "        'aq': 'aku', 'gw': 'gue', 'gua': 'gue', 'klo': 'kalau', 'kalo': 'kalau', \n",
    "        'emg': 'memang', 'dgn': 'dengan', 'skrg': 'sekarang', 'bnyk': 'banyak', \n",
    "        'byk': 'banyak', 'tdr': 'tidur', 'mkn': 'makan', 'min': 'admin',\n",
    "        'wkwk': '', 'wkwkwk': '', 'haha': '', 'hehe': ''\n",
    "    }\n",
    "    slang_dict.update(additional_slang)\n",
    "\n",
    "    stopwords_set = set()\n",
    "    try:\n",
    "        with open(stopwords_file_path, 'r', encoding='utf-8') as f:\n",
    "            stopwords_set = set(f.read().splitlines())\n",
    "        print(f\"Daftar stopwords berhasil dimuat dari '{stopwords_file_path}'. Jumlah: {len(stopwords_set)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Peringatan: File stopwords '{stopwords_file_path}' tidak ditemukan. Menggunakan stopwords default Sastrawi jika tersedia.\")\n",
    "        try:\n",
    "            remover_factory = StopWordRemoverFactory()\n",
    "            stopwords_set = set(remover_factory.get_stop_words())\n",
    "            print(f\"Menggunakan stopwords default Sastrawi. Jumlah: {len(stopwords_set)}\")\n",
    "        except Exception as e_sastrawi:\n",
    "            print(f\"Gagal memuat stopwords default Sastrawi: {e_sastrawi}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Peringatan: Gagal memuat stopwords. Error: {e}\")\n",
    "        \n",
    "    stemmer = None\n",
    "    try:\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "        print(\"Stemmer Sastrawi berhasil dibuat.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Peringatan: Gagal membuat stemmer Sastrawi. Error: {e}.\")\n",
    "        \n",
    "    text_patterns_to_remove = [\n",
    "        r\"\\(cont\\)\", r\"lanjutkan\", r\"lihat selengkapnya\", r\"see more\",\n",
    "    ]\n",
    "    regex_text_patterns = re.compile(r'|'.join(text_patterns_to_remove), re.IGNORECASE)\n",
    "\n",
    "    return slang_dict, stopwords_set, stemmer, regex_text_patterns\n",
    "\n",
    "# --- 2.2. Fungsi Preprocessing Teks ---\n",
    "def preprocess_text(text, slang_dict, stopwords_set, stemmer, regex_text_patterns, perform_stemming=True):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text_processed = str(text).lower().strip()\n",
    "    text_processed = re.sub(r'@\\w+|#\\w+', ' ', text_processed) # Hapus mention dan hashtag\n",
    "    text_processed = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', text_processed, flags=re.MULTILINE) # Hapus URL\n",
    "    text_processed = regex_text_patterns.sub(' ', text_processed) # Hapus pola teks spesifik\n",
    "    text_processed = re.sub(r'[^a-z\\s]', ' ', text_processed) # Hanya pertahankan huruf dan spasi (hapus angka & tanda baca)\n",
    "    \n",
    "    words = text_processed.split()\n",
    "    normalized_words = [slang_dict.get(word, word) for word in words]\n",
    "    text_processed = \" \".join(normalized_words)\n",
    "    \n",
    "    words = text_processed.split()\n",
    "    words = [word for word in words if word not in stopwords_set and len(word) > 1]\n",
    "    text_processed = \" \".join(words)\n",
    "    \n",
    "    if perform_stemming and stemmer:\n",
    "        try:\n",
    "            text_processed = stemmer.stem(text_processed)\n",
    "        except Exception:\n",
    "            pass # Abaikan error stemming untuk satu teks tertentu\n",
    "            \n",
    "    text_processed = re.sub(r'\\s+', ' ', text_processed).strip()\n",
    "    return text_processed\n",
    "\n",
    "# --- 2.3. Fungsi Ekstraksi Custom Features ---\n",
    "def extract_custom_features(text_series_raw):\n",
    "    \"\"\"\n",
    "    Mengekstrak custom features dari series teks mentah.\n",
    "    Mengacu pada fitur-fitur di presennn.pdf.\n",
    "    \"\"\"\n",
    "    df_features = pd.DataFrame()\n",
    "\n",
    "    # Fitur dari presennn.pdf [cite: 1]\n",
    "    df_features['text_length'] = text_series_raw.apply(lambda x: len(str(x))) # [cite: 6]\n",
    "    df_features['word_count'] = text_series_raw.apply(lambda x: len(str(x).split())) # [cite: 6]\n",
    "    df_features['avg_word_length'] = df_features['text_length'] / (df_features['word_count'] + 1e-6) # [cite: 6]\n",
    "    \n",
    "    df_features['uppercase_ratio'] = text_series_raw.apply(lambda x: sum(1 for c in str(x) if c.isupper()) / (len(str(x)) + 1e-6)) # [cite: 6]\n",
    "    df_features['question_marks'] = text_series_raw.apply(lambda x: str(x).count('?')) # [cite: 6]\n",
    "    df_features['exclamation_marks'] = text_series_raw.apply(lambda x: str(x).count('!')) # [cite: 6]\n",
    "    \n",
    "    # Menggunakan regex untuk deteksi URL yang lebih baik\n",
    "    df_features['url_count'] = text_series_raw.apply(lambda x: len(re.findall(r'http\\S+|www\\S+|https\\S+', str(x)))) # [cite: 6]\n",
    "    df_features['has_url'] = df_features['url_count'] > 0 # [cite: 6]\n",
    "    \n",
    "    # Fitur pengulangan karakter (contoh sederhana)\n",
    "    def count_char_repetition(text): # [cite: 6]\n",
    "        text = str(text)\n",
    "        count = 0\n",
    "        for char_code in range(ord('a'), ord('z') + 1):\n",
    "            char = chr(char_code)\n",
    "            if re.search(f'{char}{{5,}}', text.lower()): # Pengulangan 5 kali atau lebih [cite: 6]\n",
    "                count += 1\n",
    "        return count\n",
    "    df_features['char_repetition_gte5'] = text_series_raw.apply(count_char_repetition) # [cite: 6]\n",
    "    \n",
    "    # Fitur keberadaan pertanyaan (lebih dari sekadar tanda tanya)\n",
    "    question_words = ['apa', 'siapa', 'kapan', 'dimana', 'mengapa', 'bagaimana', 'berapa', 'kah'] # [cite: 7]\n",
    "    df_features['has_question_word'] = text_series_raw.apply(lambda x: any(word in str(x).lower() for word in question_words) or ('?' in str(x))) # [cite: 7]\n",
    "\n",
    "    # Fitur kata-kata hiperbolik/promosi (contoh sederhana)\n",
    "    hyperbolic_keywords = ['gratis', 'promo', 'diskon', 'cepat', 'mudah', 'terbaik', 'dijamin', 'untung', 'bonus', 'wajib'] # [cite: 7]\n",
    "    df_features['hyperbolic_word_count'] = text_series_raw.apply(lambda x: sum(1 for word in hyperbolic_keywords if word in str(x).lower())) # [cite: 7]\n",
    "\n",
    "    # Fitur kata perintah/imperatif (contoh sederhana)\n",
    "    imperative_keywords = ['klik', 'daftar', 'beli', 'pesan', 'kunjungi', 'download', 'ambil', 'cek'] # [cite: 7]\n",
    "    df_features['imperative_count'] = text_series_raw.apply(lambda x: sum(1 for word in imperative_keywords if word in str(x).lower())) # [cite: 7]\n",
    "\n",
    "    # Fitur indikasi gibberish (kata tanpa vokal >= 5 huruf)\n",
    "    def contains_gibberish(text): # [cite: 8]\n",
    "        words = str(text).lower().split()\n",
    "        for word in words:\n",
    "            if len(word) >= 5 and not re.search(r'[aeiou]', word): # [cite: 8]\n",
    "                return True\n",
    "        return False\n",
    "    df_features['contains_gibberish'] = text_series_raw.apply(contains_gibberish) # [cite: 8]\n",
    "\n",
    "    print(f\"Custom features berhasil diekstrak. Shape: {df_features.shape}\")\n",
    "    return df_features.astype(float) # Pastikan semua fitur numerik\n",
    "\n",
    "# --- 2.4. Fungsi Evaluasi Model ---\n",
    "def evaluate_model(model_name, y_true, y_pred, target_names):\n",
    "    print(f\"\\n--- Evaluasi untuk Model: {model_name} ---\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names, zero_division=0))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('Label Aktual')\n",
    "    plt.xlabel('Label Prediksi')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# BAGIAN 3: FUNGSI PIPELINE UTAMA\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification_pipeline():\n",
    "    print(\"--- MEMULAI PIPELINE DETEKSI SPAM MULTIKELAS ---\")\n",
    "\n",
    "    # --- 1. Pemuatan dan Persiapan Data Awal ---\n",
    "    print(\"\\n[Tahap 1] Pemuatan & Persiapan Data...\")\n",
    "    try:\n",
    "        df = pd.read_csv(DATASET_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Dataset '{DATASET_PATH}' tidak ditemukan.\")\n",
    "        return\n",
    "    \n",
    "    if 'ulasan' not in df.columns or 'kategori' not in df.columns:\n",
    "        print(\"ERROR: Kolom 'ulasan' dan 'kategori' tidak ditemukan di dataset.\")\n",
    "        return\n",
    "    df.dropna(subset=['ulasan', 'kategori'], inplace=True) # Hapus baris jika ada NaN di kolom penting\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['kategori_encoded'] = label_encoder.fit_transform(df['kategori'])\n",
    "    label_mapping = {i: class_name for i, class_name in enumerate(label_encoder.classes_)}\n",
    "    print(\"Pemetaan Label:\", label_mapping)\n",
    "\n",
    "    X_raw = df['ulasan']\n",
    "    y = df['kategori_encoded']\n",
    "\n",
    "    X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "        X_raw, y, test_size=TEST_SET_SIZE, random_state=RANDOM_STATE_SEED, stratify=y\n",
    "    )\n",
    "    print(f\"Data dibagi: {len(X_train_raw)} latih, {len(X_test_raw)} uji.\")\n",
    "\n",
    "    # --- 2. Persiapan Alat Preprocessing ---\n",
    "    print(\"\\n[Tahap 2] Menyiapkan Alat Preprocessing...\")\n",
    "    slang_map, stopwords_set, sastrawi_stemmer, regex_patterns = setup_preprocessing_tools(SLANG_FILE_PATH, STOPWORDS_FILE_PATH)\n",
    "\n",
    "    # --- 3. Preprocessing Teks ---\n",
    "    print(\"\\n[Tahap 3] Preprocessing Teks...\")\n",
    "    print(f\"Preprocessing X_train_raw (stemming={PERFORM_STEMMING_DEFAULT})...\")\n",
    "    X_train_processed = X_train_raw.apply(\n",
    "        lambda text: preprocess_text(text, slang_map, stopwords_set, sastrawi_stemmer, regex_patterns, PERFORM_STEMMING_DEFAULT)\n",
    "    )\n",
    "    print(f\"Preprocessing X_test_raw (stemming={PERFORM_STEMMING_DEFAULT})...\")\n",
    "    X_test_processed = X_test_raw.apply(\n",
    "        lambda text: preprocess_text(text, slang_map, stopwords_set, sastrawi_stemmer, regex_patterns, PERFORM_STEMMING_DEFAULT)\n",
    "    )\n",
    "    print(\"Preprocessing teks selesai.\")\n",
    "\n",
    "    # --- 4. Rekayasa Fitur ---\n",
    "    print(\"\\n[Tahap 4] Rekayasa Fitur...\")\n",
    "    # 4.1 Custom Features\n",
    "    print(\"Mengekstrak custom features untuk data latih...\")\n",
    "    X_train_custom_features_df = extract_custom_features(X_train_raw)\n",
    "    print(\"Mengekstrak custom features untuk data uji...\")\n",
    "    X_test_custom_features_df = extract_custom_features(X_test_raw)\n",
    "    \n",
    "    # Standarisasi custom features (opsional tapi direkomendasikan jika digabung dengan TF-IDF untuk SVM)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_custom_features_scaled = scaler.fit_transform(X_train_custom_features_df)\n",
    "    X_test_custom_features_scaled = scaler.transform(X_test_custom_features_df)\n",
    "\n",
    "    # 4.2 TF-IDF Features\n",
    "    print(\"Mengekstrak fitur TF-IDF...\")\n",
    "    # Menggunakan parameter dari presentasi Anda [cite: 1] (misal max_features, ngram_range)\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=10000, # [cite: 9] (Contoh, bisa di-tune)\n",
    "        ngram_range=(1, 2) # [cite: 9] (Contoh, bisa di-tune)\n",
    "    ) \n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_processed)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test_processed)\n",
    "    print(f\"Shape matriks TF-IDF data latih: {X_train_tfidf.shape}\")\n",
    "\n",
    "    # 4.3 Kombinasi Fitur\n",
    "    print(\"Menggabungkan fitur TF-IDF dan custom features...\")\n",
    "    # Menggabungkan matriks sparse TF-IDF dengan custom features yang sudah di-scaled (dan dijadikan sparse)\n",
    "    from scipy.sparse import csr_matrix\n",
    "    X_train_final = hstack([X_train_tfidf, csr_matrix(X_train_custom_features_scaled)]).tocsr()\n",
    "    X_test_final = hstack([X_test_tfidf, csr_matrix(X_test_custom_features_scaled)]).tocsr()\n",
    "    print(f\"Shape matriks fitur final data latih: {X_train_final.shape}\")\n",
    "    print(f\"Shape matriks fitur final data uji: {X_test_final.shape}\")\n",
    "\n",
    "    # --- 5. Penanganan Ketidakseimbangan Kelas (Opsional, contoh jika diperlukan) ---\n",
    "    # print(\"\\n[Tahap 5] Penanganan Ketidakseimbangan Kelas (jika diperlukan)...\")\n",
    "    # if USE_SMOTE:\n",
    "    #     from imblearn.over_sampling import SMOTE\n",
    "    #     smote = SMOTE(random_state=RANDOM_STATE_SEED)\n",
    "    #     X_train_final, y_train = smote.fit_resample(X_train_final, y_train)\n",
    "    #     print(f\"Data latih setelah SMOTE: {X_train_final.shape}, Distribusi y_train: {np.bincount(y_train)}\")\n",
    "    # else:\n",
    "    #     print(\"SMOTE tidak digunakan.\")\n",
    "\n",
    "    # --- 6. Pelatihan Model ---\n",
    "    print(\"\\n[Tahap 6] Pelatihan Model...\")\n",
    "    models_to_train = {\n",
    "        \"Naive Bayes\": MultinomialNB(),\n",
    "        # SVM lebih baik dengan data yang di-scale jika ada fitur dense, atau jika kernel non-linear\n",
    "        # Karena kita menggabung sparse TF-IDF dengan custom features (yang kita scale), SVM tetap relevan\n",
    "        \"SVM\": SVC(probability=True, random_state=RANDOM_STATE_SEED, class_weight='balanced') # class_weight untuk imbalance\n",
    "    }\n",
    "    \n",
    "    trained_models = {}\n",
    "    for model_name, model_instance in models_to_train.items():\n",
    "        print(f\"\\nMelatih model {model_name}...\")\n",
    "        try:\n",
    "            # Naive Bayes tidak bisa menerima nilai negatif, jadi jika custom features di-scale bisa jadi masalah\n",
    "            # Untuk Naive Bayes, mungkin lebih baik menggunakan custom features tanpa scaling negatif atau FeatureUnion\n",
    "            # Atau hanya menggunakan TF-IDF untuk NB sebagai baseline awal.\n",
    "            # Untuk contoh ini, kita coba latih NB dengan TF-IDF saja dulu.\n",
    "            if model_name == \"Naive Bayes\":\n",
    "                 # Cek apakah ada nilai negatif di TF-IDF (seharusnya tidak ada, tapi custom features yang di-scale bisa)\n",
    "                 # Karena TF-IDF non-negatif, MultinomialNB cocok. Custom features yang di-scale mungkin butuh GaussianNB atau diskritisasi.\n",
    "                 # Untuk simplifikasi, kita latih NB hanya dengan TF-IDF jika ada masalah dengan gabungan fitur.\n",
    "                 # Namun, hstack seharusnya aman jika custom_features_scaled juga non-negatif (atau jika NB bisa handle).\n",
    "                 # MultinomialNB mengharapkan fitur non-negatif. Scaled custom features bisa negatif.\n",
    "                 # Solusi: 1. Gunakan TF-IDF saja untuk NB, 2. Gunakan model lain (misal SVM) untuk gabungan, 3. MinMaxScaler untuk custom features.\n",
    "                 # Kita coba dengan TF-IDF saja untuk NB agar lebih aman.\n",
    "                print(f\"Melatih Naive Bayes hanya dengan fitur TF-IDF untuk menghindari nilai negatif...\")\n",
    "                model_instance.fit(X_train_tfidf, y_train)\n",
    "            else:\n",
    "                model_instance.fit(X_train_final, y_train)\n",
    "            trained_models[model_name] = model_instance\n",
    "            print(f\"Model {model_name} berhasil dilatih.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR saat melatih {model_name}: {e}\")\n",
    "\n",
    "\n",
    "    # --- 7. Evaluasi Model ---\n",
    "    print(\"\\n[Tahap 7] Evaluasi Model...\")\n",
    "    class_names_list = list(label_encoder.classes_)\n",
    "    for model_name, model in trained_models.items():\n",
    "        if model_name == \"Naive Bayes\": # Evaluasi NB dengan TF-IDF saja\n",
    "            y_pred = model.predict(X_test_tfidf)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test_final)\n",
    "        evaluate_model(model_name, y_test, y_pred, target_names=class_names_list)\n",
    "\n",
    "    # --- 8. Tuning Hyperparameter (Placeholder) ---\n",
    "    print(\"\\n[Tahap 8] Tuning Hyperparameter (TODO)...\")\n",
    "    # Contoh untuk SVM:\n",
    "    # param_grid_svm = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['rbf', 'linear']}\n",
    "    # grid_svm = GridSearchCV(SVC(probability=True, random_state=RANDOM_STATE_SEED, class_weight='balanced'), \n",
    "    #                         param_grid_svm, cv=3, scoring='f1_weighted', verbose=1)\n",
    "    # grid_svm.fit(X_train_final, y_train)\n",
    "    # print(\"Best SVM params:\", grid_svm.best_params_)\n",
    "    # y_pred_svm_tuned = grid_svm.predict(X_test_final)\n",
    "    # evaluate_model(\"SVM (Tuned)\", y_test, y_pred_svm_tuned, target_names=class_names_list)\n",
    "    # trained_models[\"SVM (Tuned)\"] = grid_svm.best_estimator_\n",
    "\n",
    "\n",
    "    # --- 9. Penyimpanan Model Terbaik (Contoh) ---\n",
    "    print(\"\\n[Tahap 9] Penyimpanan Model (TODO)...\")\n",
    "    # if \"SVM (Tuned)\" in trained_models:\n",
    "    #     best_model_object = trained_models[\"SVM (Tuned)\"]\n",
    "    #     model_filename = \"best_spam_multiclass_model.pkl\"\n",
    "    # elif \"SVM\" in trained_models: # Fallback ke SVM biasa jika tuning tidak dilakukan\n",
    "    #     best_model_object = trained_models[\"SVM\"]\n",
    "    #     model_filename = \"svm_spam_multiclass_model.pkl\"\n",
    "    # else: # Atau pilih model lain\n",
    "    #     best_model_object = trained_models.get(\"Naive Bayes\")\n",
    "    #     model_filename = \"nb_spam_multiclass_model.pkl\"\n",
    "\n",
    "    # if best_model_object:\n",
    "    #     try:\n",
    "    #         with open(model_filename, 'wb') as f:\n",
    "    #             pickle.dump({\n",
    "    #                 'model': best_model_object,\n",
    "    #                 'tfidf_vectorizer': tfidf_vectorizer,\n",
    "    #                 'custom_feature_scaler': scaler, # Simpan juga scaler untuk custom features\n",
    "    #                 'label_encoder': label_encoder,\n",
    "    #                 'label_mapping': label_mapping,\n",
    "    #                 'custom_feature_columns': list(X_train_custom_features_df.columns) # Simpan nama kolom custom features\n",
    "    #             }, f)\n",
    "    #         print(f\"Model terbaik dan komponennya disimpan ke '{model_filename}'\")\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error saat menyimpan model: {e}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- PIPELINE SELESAI ---\")\n",
    "    return df, trained_models # Mengembalikan dataframe dan model untuk inspeksi jika perlu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# BAGIAN 4: EKSEKUSI UTAMA\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MEMULAI PIPELINE DETEKSI SPAM MULTIKELAS ---\n",
      "\n",
      "[Tahap 1] Pemuatan & Persiapan Data...\n",
      "Pemetaan Label: {0: 'explicit_spam', 1: 'genuine_review', 2: 'irrelevant_content'}\n",
      "Data dibagi: 12000 latih, 3000 uji.\n",
      "\n",
      "[Tahap 2] Menyiapkan Alat Preprocessing...\n",
      "Kamus slang berhasil dimuat dari '../../dataset/slang_indo.csv'. Jumlah entri: 1250\n",
      "Daftar stopwords berhasil dimuat dari '../../dataset/stopwords-id.txt'. Jumlah: 758\n",
      "Stemmer Sastrawi berhasil dibuat.\n",
      "\n",
      "[Tahap 3] Preprocessing Teks...\n",
      "Preprocessing X_train_raw (stemming=False)...\n",
      "Preprocessing X_test_raw (stemming=False)...\n",
      "Preprocessing teks selesai.\n",
      "\n",
      "[Tahap 4] Rekayasa Fitur...\n",
      "Mengekstrak custom features untuk data latih...\n",
      "Custom features berhasil diekstrak. Shape: (12000, 13)\n",
      "Mengekstrak custom features untuk data uji...\n",
      "Custom features berhasil diekstrak. Shape: (3000, 13)\n",
      "Mengekstrak fitur TF-IDF...\n",
      "Shape matriks TF-IDF data latih: (12000, 10000)\n",
      "Menggabungkan fitur TF-IDF dan custom features...\n",
      "Shape matriks fitur final data latih: (12000, 10013)\n",
      "Shape matriks fitur final data uji: (3000, 10013)\n",
      "\n",
      "[Tahap 6] Pelatihan Model...\n",
      "\n",
      "Melatih model Naive Bayes...\n",
      "Melatih Naive Bayes hanya dengan fitur TF-IDF untuk menghindari nilai negatif...\n",
      "Model Naive Bayes berhasil dilatih.\n",
      "\n",
      "Melatih model SVM...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_result, models_result = run_classification_pipeline()\n",
    "    # Anda bisa menambahkan kode di sini untuk interaksi lebih lanjut dengan df_result atau models_result\n",
    "    # jika dijalankan dalam environment interaktif.\n",
    "    print(\"\\nUntuk mengakses hasil (jika dijalankan sebagai skrip utama dan ingin diinspeksi):\")\n",
    "    print(\"df_result berisi DataFrame awal dengan kolom encoded.\")\n",
    "    print(\"models_result adalah dictionary berisi model-model yang sudah dilatih.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
